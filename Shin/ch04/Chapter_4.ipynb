{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 Part에서는 **신경망 학습**에 대해 알아보겠습니다 **학습**이란 훈련 데이터로 부터 최적의 매개변수 값을 자동으로 획득하는 것입니다.\n",
    "이렇게 이야기하니 너무 딱딱하고 어려운데 쉽게 이야기하면 훈련 데이터를 통해서 우리가 해결하려고 하는 문제를 잘 풀 수 있는 파라미터(Weight,bias)를 찾는 \n",
    "과정입니다. 그렇다면 최적의 파라미터값을 찾아가는 기준은 무엇일까요? 바로 **손실함수** 입니다. 이번 장에서는 **손실함수**를 사용하여 최적의 파라미터 값을 찾는 \n",
    "**학습** 에 관련된 부분을 학습합니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 데이터에서 학습한다!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 데이터 주도 학습"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 신경망 학습을 본격적으로 알아보기 전에 한가지 기억해야할 것이 하나 있습니다. **기계학습에서 가장 중요한것은 데이터라는 점!** 입니다. \n",
    "기계학습은 데이터는 수집한 데이터로부터 패턴을 찾습니다. 예를 들어 숫자 5 사진을 보고 5를 분류하는 모델을 생성한다고 생각합니다.\n",
    "5라는 숫자는 아래쪽이 둥글다,왼쪽 위 모서리가 각져있다 등 다양한 특징들을 찾을 수 있습니다. 이러한 다양한 특징들을 학습한다면 \n",
    "숫자 5를 분류하는 모델을 생성할 수 있습니다. **특징(Feature)** 이란 입력된 데이터에서 중요한 데이터를 의미합니다. \n",
    " \n",
    "기계학습은 이러한 특징들을 넣어 학습합니다. 만약 기계학습에 문제에 적절한 **특징(Feature)** 을 넣지 않는다면 좋은 결과를 얻을 수 없습니다. 이처럼 기계학습에서는\n",
    "중요한 특징들을 추출하는 것이 중요합니다. 반면 딥러닝은 입력데이터를 그대로 입력받아 스스로 **특징(Feature)** 를 추출하여 End-to-End 방식으로 학습합니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 훈련 데이터와 시험 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 기계학습이라는 큰 틀 안에 딥러닝이 존재합니다. 그래서 편의상 기계학습과 딥러닝이 같은 의미로 나올 수 있으니 참고해주세요.  \n",
    "\n",
    "기계학습 문제는 훈련 데이터와 테스트 데이터로 나누어 학습과 실험을 진행합니다. 훈련 데이터로만 학습하여 훈련 데이터에 대한 최적의 매개변수 값을 찾고\n",
    "테스트 데이터를 통해 훈련한 모델을 평가합니다. 왜 나누어서 학습과 실험을 진행하는 걸까요? 바로 범용능력을 제대로 측정하기 위해서입니다.  \n",
    "범용능력이란? -> 한번도 보지 못한 데이터에 대해서 올바르게 풀어내는 능력  \n",
    "만약 테스트 데이터까지 학습에 사용한다면 범용능력을 평가하기가 어려워지기 때문에 나누는 것입니다.\n",
    "그리고 한가지 더 알아야할 것이 있습니다. 바로 **오버피팅(Overfitting)** 입니다. 모델이 문제를 풀어내는 능력을 기른것이 아니라 \n",
    "정답을 모두 외워버렷다고 이해하시면 됩니다. 이런 경우 당연히 범용능력이 떨어지겠죠? 이처럼 기계학습에서는 범용능력을 키우고 오버피팅을 피하는것이 중요한 과제입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 손실함수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습에서는 현재의 상태를 \"하나의 지표\"로 표현하고 그 지표를 가장 좋게하는 최적의 가중치 매개변수 값을 탐색하는 것입니다.  \n",
    "이때 신경망 학습에서 사용되는 \"하나의 지표\"를 **손실함수(Loss function)** 라고 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 오차제곱합(Sum of Squares Error ,SSE)\n",
    "- 오차제곱합은 가장 많이 쓰이는 손실 함수중 하나입니다.\n",
    "- 모델이 추론한 값과 실제 label 값을 빼서 제곱한 것들의 합을 1/2로 나누어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_error(y_hat,y):\n",
    "    return np.sum((y_hat-y)**2) * 0.5 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차제곱합을 식을 기반으로 구현하였습니다. 그렇다면 이것이 어떻게 작동되는지 한번 확인해보기 전에  \n",
    "원-핫 인코딩에대해서 이야기하고 바로 예제를 진행해보겠습니다.  \n",
    "원-핫 인코딩은 한 원소만 1로 표현하고 나머지는 0으로 표현하는 인코딩 방법입니다.   \n",
    "만약 [0,0,1,0] 인 경우에는 숫자 0~3을 표현한다고했을때 2에 해당하는 원-핫 인코딩인것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat1 : 0.16\n",
      "y_hat2 : 0.56\n"
     ]
    }
   ],
   "source": [
    "y = np.array([0,0,1,0,0]) # one-hot -> 2\n",
    "\n",
    "# softmax를 통과한 출력 -> 3번째 인덱스의 값이 가장 큼 -> 2\n",
    "y_hat1 = np.array([0.1,0.2,0.5,0.1,0.1])\n",
    "\n",
    "# 0번째 인덱스의 값이 가장 큼 -> 0\n",
    "y_hat2 = np.array([0.5,0.2,0.1,0.1,0.1] )\n",
    "\n",
    "print('y_hat1 :',sum_of_squares_error(y_hat1,y))\n",
    "print('y_hat2 :',sum_of_squares_error(y_hat2,y))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차제곱합으로 손실값을 계산한결과 label와 index와 추론값의 가장 큰 값의 index가 동일한 출력의 손실이 더 적은 것으로 확인이 가능합니다.  \n",
    "그리고 label의 해당하는 index의 값을 크게 만드는 방향으로 학습하면 손실함수의 값이 감소하는 방향일 것입니다.   \n",
    "이처럼 손실함수는 신경망 학습의 하나의 지표로서 활용이 가능합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차(Cross Entropy Error,CEE)\n",
    "- 오차제곱합와 같이 많이 사용되는 손실함수 입니다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_hat,y):\n",
    "    delta = 1e-7 # 값이 너무 작아지는 것을 방지\n",
    "    return -np.sum(y*np.log(y_hat+delta)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "식을 그림을 그리면서 하나씩 설명하겠습니다. 먼저 y가 원-핫 인코딩으로 구현이 되어있다고 가정하고, \n",
    "y_hat은 최종 출력계층의 활성화 함수 Softmax 를 통과했다고 가정해보겠습니다. 그러면 아래 그림과 같은 손실함수 값을 얻을 수 있고\n",
    "로그함수를 비교해봣을때 label에 해당하는 index의 출력값을 키우는 방향으로 학습하면 된다는 방향을 확인해 볼 수 있습니다.  \n",
    "\n",
    "이제 똑같은 예제로 한번 손실함수의 값을 확인해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat1 : 0.6931469805599654\n",
      "y_hat2 : 2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "y = np.array([0,0,1,0,0]) # one-hot -> 2\n",
    "\n",
    "# softmax를 통과한 출력 -> 3번째 인덱스의 값이 가장 큼 -> 2\n",
    "y_hat1 = np.array([0.1,0.2,0.5,0.1,0.1])\n",
    "\n",
    "# 0번째 인덱스의 값이 가장 큼 -> 0\n",
    "y_hat2 = np.array([0.5,0.2,0.1,0.1,0.1] )\n",
    "\n",
    "print('y_hat1 :',cross_entropy_error(y_hat1,y))\n",
    "print('y_hat2 :',cross_entropy_error(y_hat2,y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차제곱합때보다 좀 더 확연하게 손실함수 값의 차이가 나는 것을 볼 수 있습니다. \n",
    "실제로 오차제곱합은 주로 Regression Task에서 사용하고  \n",
    "교차 엔트로피의 경우 Classification Task에서 손실함수로 많이 사용됩니다.\n",
    "\n",
    "> 추가로 보통 오차제곱합과 교차엔트로피 등 손실함수를 사용할 때는 데이터의 갯수 N을 최종값에 나누어 평균 손실 함수를 구합니다.  \n",
    "> 평균 손실 함수를 구하는 이유는 데이터 개수와 상관없이 언제든 통일된 지표를 얻기 위함입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습\n",
    "- 미니배치 학습 - 훈련데이터로 부터 일부를 골라 학습하는 방법\n",
    "- 왜 미니배치 학습을 사용할까? \n",
    "  - 데이터가 한번에 처리하기 너무 큰 양일 수 있어서 \n",
    "  - 모든 데이터를 학습에 반복하여 사용할 경우 연산이 많아 효율적이지 않을 수 있어서\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 이유로 미니배치학습을 많이 사용합니다. 그러면 이제 미니배치를 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train : (60000, 784)\n",
      "x_test : (10000, 784)\n",
      "y_train : (60000, 10)\n",
      "y_test : (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,y_train) , (x_test,y_test) = load_mnist(normalize=True,flatten=True,one_hot_label=True)\n",
    "\n",
    "print(\"x_train :\",x_train.shape) # 학습 이미지 \n",
    "print(\"x_test :\",x_test.shape) # 테스트 이미지\n",
    "print(\"y_train :\",y_train.shape) # 학습 이미지 abel\n",
    "print(\"y_test :\",y_test.shape) # 테스트 이미지 label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치를 구현하기 위한 함수로 `np.random.choice` 를 사용할 수 있습니다. 이 함수는 지정된 범위안에서 지정한 만큼의 데이터를\n",
    "무작위로 뽑아줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|x_batch|: (10, 784)\n",
      "|y_batch|: (10, 10)\n"
     ]
    }
   ],
   "source": [
    "train_cnt = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_cnt,batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "y_batch = y_train[batch_mask]\n",
    "\n",
    "print(\"|x_batch|:\",x_batch.shape)\n",
    "print(\"|y_batch|:\",y_batch.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기\n",
    "미니배치를 지원하는 교차 엔트로피 오차를 구현해보도록 하겠습니다. 그리 어렵지 않습니다 추가된 개념이라곤 데이터가 미니배치 단위로 들어오는 것 뿐입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_hat,y):\n",
    "\n",
    "    if y.ndim == 1:\n",
    "        y_hat = y_hat.reshape(1,y_hat.shape[0])\n",
    "        y = y.reshape(1,y.shape[0])\n",
    "\n",
    "    batch_size = y_hat.shape[0]\n",
    "    delta = 1e-7\n",
    "    return np.sum(y*np.log(y_hat+delta)) / batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음은 label이 원-핫 인코딩으로 이루어져있지 않을때의 배치용 교차 엔트로피를 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_hat,y):\n",
    "\n",
    "    if y.ndim == 1:\n",
    "        y_hat = y_hat.reshape(1,y_hat.shape[0])\n",
    "        y = y.reshape(1,y.shape[0])\n",
    "\n",
    "    batch_size = y_hat.shape[0]\n",
    "    delta = 1e-7\n",
    "    return np.sum(np.log(y_hat[np.arange(batch_size),y]+delta)) / batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.sum(y*np.log(y_hat+delta))` 에서 `np.sum(np.log(y_hat[np.arange(batch_size),y]+delta))`로 바뀐 이유는   \n",
    "label(y)의 경우 원-핫 인코딩 벡터일때는 정답에 해당하는 label index를 알기위해 사용됬지만 원-핫 인코딩이 아닌 2,3 과 같이 숫자인 경우에  \n",
    "이를 곱해줄 이유가 없어졌기 떄문입니다 추론된 값(y_hat)에서 몇번째 데이터의 label index에 값만 가져오면 되기 때문입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 왜 손실함수를 설정하는가?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숫자 인식의 경우 우리의 궁극적인 목표는 높은 **\"정확도\"** 를 이끌어내는 매개변수를 찾는 것 입니다. 그런데 왜 정확도라는 지표를 사용하지 않고  \n",
    "손실함수를 사용하는걸까요?  \n",
    "\n",
    "이러한 의문은 바로 신경망의 학습에서의 **\"미분\"** 의 역할을 안다면 쉽게 해결할 수 있습니다.  \n",
    "**신경망의 학습은 손실함수(Loss function)의 값을 가능한 작게 만드는 최적의 매개변수(Weight&Bias)를 찾는 것입니다.**   \n",
    "매개변수의 미분을 계산하고 그 미분값을 단서로 매개변수의 값을 손실함수(Loss function)이 작아지는 방향으로 갱신하는 과정을 반복합니다. \n",
    "\n",
    "어떤 가중치 매개변수를 손실함수에 대해 미분을 해보았을 때 구한 미분값(편의상 기울기,그레디언트 라고 하겠습니다)을 가지고  \n",
    "만약 그레디언트값이 양수일 경우 음의 방향으로 매개변수를 갱신한다면 손실함수의 값이 낮아질 것이고  \n",
    "만약 그레디언트값이 음수일 경우 양의 방향으로 매개변수를 갱신한다면 손실함수의 값이 낮아질 것입니다.  \n",
    "그리고 만약 그레디언트값이 0일 경우 어느쪽으로 움직여도 손실함수의 값은 줄어들지 않기 때문에 갱신이 이루어지지 않을겁니다.  \n",
    "\n",
    "우리가 손실함수를 신경망 학습에서의 지표를 정확도가 아닌 손실함수를 사용하는 이유는 미분값을 통해 매개변수를 조금씩 갱신하면서 최적의 매개변수를 찾기 위함이라는 거죠\n",
    "만약 정확도를 지표로 신경망을 학습한다고 가정해보겠습니다. 만약 정확도가 33%가 나왔다고 했을 때 우리는 이 정확도를 높히기 위해 매개변수들을 갱신할 것입니다.\n",
    "그런데 매개변수는 조금씩 업데이트가 되는데 정확도라는 지표가 매개변수가 조금 변경되었다고 해서 올라갈까요? 그리고 만약 올라가더라고 34,37,40% 와 같이 띄엄띄엄\n",
    "업데이트 될 가능성이 높습니다. 즉 정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않고 반응이 있더라도 그 값이 불연속적으로 갑자기 변화합니다.\n",
    "이러한 이유는 정확도는 대부분의 장소에서 미분값이 0이기 때문입니다. 그렇기 때문에 우리는 매개변수의 미소한 변화에도 값이 연속적으로 변하는 손실함수를 신경망 학습에 \n",
    "지표로 사용하는 것입니다.\n",
    "\n",
    "> 추가적으로 우리가 활성화 함수중 계단함수를 잘 사용하지 않는 것도 계단함수는 거의 대부분의 장소에서 미분값이 0입니다.  \n",
    "> (미분에 대한 개념이 부족하시면 밑에 **\"4.3 수치미분\"** 을 먼저 보고 오시는것이 좋습니다)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 수치 미분"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사법은 그레디언트(경사)값을 기준으로 나아갈 방향을 정합니다. 만약 손실함수를 낮추는 방법을 경사법으로 풀면  \n",
    "어떤 매개변수의 손실함수의 그레디언트 값이 양수이면 음의 방향으로 음수이면 양의 방향으로 경사법의 방법으로 가면 됩니다.\n",
    "이 파트에서는 그레디언트가 무엇인지 어떤 성질이 있는지 알아보도록 하겠습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 미분\n",
    "> 미분에 대해서 학생 때 배운 개념을 알고 계신다면 넘어가셔도 좋습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 한 마라톤선수가 10분동안 2km를 달렸다고 해봅시다. 그러면 이 마라톤 선수는 ${2 km \\over 10 분} =0.2km/min$ 로 1분당 0.2km만큼의 속도로  \n",
    "뛰었다고 해석할 수 있습니다. 달린 거리가 시간에 대해서 얼마나 변했는가를 계산했습니다. 다만 우리는 \"평균속도\"를 구한 것입니다.  \n",
    "  \n",
    "**\"미분\"** 이란 ***'특정 순간'*** 의 변화량을 뜻합니다. 즉 정말 어느 한 순간 아주 짧은 순간의 변화량을 의미합니다.\n",
    "우리는 이것을 수식으로 다음과 같이 나타낼 수 있습니다.  \n",
    "$${df(x) \\over dx} =\\displaystyle\\lim_{h \\rarr 0} \\space {f(x+h) - f(x) \\over h}$$  \n",
    "\n",
    "그렇다면 이것을 코드로 한번 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-50\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 미분을 수치미분(numerical differentiation)이라고 합니다. 그래서 함수 이름을 numerical_diff로 하였습니다.  \n",
    "그리고 구현시 한가지 알아야 할 점이 있습니다. 우리는 h라는 값이 0에 가까운 아주 작은 값 `1e-50`으로 지정했습니다. 하지만\n",
    "이렇게 값이 작을 경우 **\"반올림오차(rounding error)\"** 라는 문제가 발생하게 됩니다. 다시말하면 너무 작은 값(예를 들면 소수점 8자리 이하의 값)들이 생략되어\n",
    "최종 계산 결과에 오차가 발생하는 경우입니다. 그래서 파이썬으로 구현시에는 보통 이런 문제가 발생하지 않게 하기 위해서 **h값을 `1e-4`** 로 사용합니다.(좋은 결과를 얻는다고 알려져 있습니다) \n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 또 한가지 알아야 할것이 있습니다. 우리가 위쪽에서 구현한 코드는 전방차분을 이용한 수치미분입니다. 그러나 전방차분을 사용해 수치미분을 할 경우\n",
    "우리가 알고 싶은 진정한 미분값과 수치 미분값 사이 차이가 발생하게 됩니다. 그 이유는 아래의 그림과 같이 $f(x+h)$ 은 $f(x)$ 은 h값이라는 작은 차이가 있기 때문입니다. 그렇기 때문에 진정한 접선과 근사로 구한 접선 그림처럼 차이가 발생하게 됩니다. 그래서 이러한 분제를 해결하기 위해 **중앙차분** 또는 **중심차분** 방법을 사용합니다. 코드를 통해 한번 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4 # 반올림오차 방지\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${df(x) \\over dx} = \\displaystyle\\lim_{h\\rarr0}{f(x+h) - f(x-h)\\over 2h}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 식 처럼 중앙차분을 하게된다면 전방차분을 사용했을 때에 수치미분값과 진정한 미분값 사이의 차이를 줄일 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 편미분"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편미분은 변수가 여럿인 함수에 대한 미분입니다. 편미분 기호로는 $d$ 대신 $\\partial$ 을 사용합니다. 어렵게 생각하실 필요 없습니다.  \n",
    "변수가 여럿인 함수에 대해서 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한 상태로 미분을 진행하면 됩니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = x^{2}_1 + x^{2}_2 + 3$$  \n",
    "$${\\partial f(x)\\over \\partial x_1} = 2x_1$$\n",
    "$${\\partial f(x)\\over \\partial x_2} = 2x_2$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 기울기(Gradient)\n",
    "> **Review**  \n",
    "> 저번 파트에서는 미분에 대해서 배웠습니다. 그럼 한번 복습을 해보도록 하겠습니다. 우리는 왜 미분을 배웠고 미분값을 구해야하는걸까요?  \n",
    "> 바로 신경망 학습은 손실함수를 가능한 작게하는 최적의 매개변수(Weight&Bias)를 구하는 것이 목표이고  \n",
    "> 매개변수에 대한 손실함수의 미분값(그레디언트)을 구하여 경사법으로 손실함수가 가능한 작아지는 방향으로 매개변수를 갱신하여 최적의 매개변수를 찾기 위함입니다.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그레디언트란 모든 변수의 편미분을 벡터로 정리한 것입니다. 위의 공식을 예시로 그레디언트를 정의한다고 하면 그레디언트는 $({\\partial f \\over \\partial x_1},{\\partial f \\over \\partial x_2}) $ 입니다. 이를 파이썬코드로 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x): # 설명 \n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.shape[0]):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1-fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.zeros_like(x) : x와 shape이 같고 원소가 모두 0인 numpy array를 만듭니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x)$ 를 하나 정의해서 한번 결과가 제대로 나오는지 확인 해보도록 하겠습니다.\n",
    "$$f(x) = x^{2}_1 + 3x^{2}_2$$    \n",
    "$${\\partial f\\over \\partial x_1} = 2x_1$$\n",
    "$${\\partial f\\over \\partial x_1} = 6x_2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6. 24.]\n"
     ]
    }
   ],
   "source": [
    "def function_1(x):\n",
    "    return (x[0]**2) + 3*(x[1]**2)\n",
    "\n",
    "print(numerical_diff(function_1,np.array([3.,4.])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_1=3,x_2=4$ 일때 그레디언트가 올바르게 나오는것을 확인 할 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***그레디언트가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향*** 이기 때문에  \n",
    "우리는 신경망 학습에서 그레디언트를 사용하여 손실함수의 값을 가능한 작게 하는 매개변수의 값을 찾습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 경사법(경사하강법)\n",
    "우리는 신경망 학습을 통해 최적의 매개변수 값을 찾습니다. 이때 최적이라는 것은 손실함수가 최솟값이 될 때를 이야기합니다.  \n",
    "그러나 문제는 대부분의 문제의 손실함수는 매우 복잡하고 광대합니다 그래서 최적의 매개변수 값이 어디에 있는지 짐작하기 어렵습니다.  \n",
    "그렇기 때문에 우리는 그레디언트를 이용하여 손실함수의 최솟값을 찾습니다. 이 방법을 **\"경사법(경사하강법)\"** 이라고 합니다.\n",
    "\n",
    "물론 그레디언트가 가르키는 곳이 무조건 최소값으로 가는 방향이라고는 보장할 수 없고 실제 문제에서 대부분 그레디언트가 가리키는 방향이 최솟값  \n",
    "인 경우는 드뭅니다. 그러나 그레디언트가 가리키는 방향으로 가야 함수의 값을 줄일 수 있습니다.\n",
    "\n",
    "즉 정리하면 다음과 같습니다.\n",
    "- 경사법은 그레디언트를 활용하여 손실함수의 값을 줄이는 방법입니다.\n",
    "- 그레디언트가 가리키는 방향이 무조건 최솟값으로 가는 방향이라고 하기는 어렵다.\n",
    "- 경사법은 현 위치에서 그레디언트 방향으로 일정 거리만큼 이동합니다.\n",
    "- \\+ 대부분 신경망 학습에는 경사법(경사하강법)을 많이 사용합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사법(경사하강법) 수식\n",
    "$$ x_n = x_n - \\eta {\\partial f \\over \\partial x_n}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습률(Learning Rate,$\\eta$) : 매개변수의 값을 얼마나 갱신할건지를 정하는 하이퍼파라미터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 이제 경사법(경사하강법)을 파이썬 코드로 구현해보겠습니다.\n",
    "- `gradient_decent(f,init_x,lr=0.01,step_num=100)`\n",
    "  - `f` : 손실함수\n",
    "  - `init_x` : 초기 입력값\n",
    "  - `lr(learning rate)` : 학습률\n",
    "  - `step_num` : 학습횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_diff(f,x)\n",
    "        x = x - lr*grad\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률(Learning Rate)은 매개변수가 얼마나 갱신될건지를 정하는 하이퍼파라미터입니다.  \n",
    "학습률의 크기에 따라 학습이 어떻게 이루어지는 매개변수가 어떻게 업데이트 되는지 한번 예제를 통해 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case1:  [-3.00000027  3.99999984  1.99999999]\n",
      "case2:  [-1.50009858e+14  6.08400002e+03 -1.80001693e+01]\n"
     ]
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**3+2*(x[1]**2)+x[2]\n",
    "\n",
    "init_x = np.array([-3.,4.,2.])\n",
    "\n",
    "case1=gradient_descent(function_2,init_x,lr=1e-10,step_num=100)\n",
    "case2=gradient_descent(function_2,init_x,lr=10,step_num=100)\n",
    "\n",
    "print('case1: ',case1)\n",
    "print('case2: ',case2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`case1`의 경우 학습률이 너무 작아 초기 입력값(`init_x`)에 비해 변화가 아주 적습니다.  \n",
    "`case2`의 경우 학습률이 너무 커 값이 발산해버린것을 확인할 수 있습니다.  \n",
    "이처럼 우리는 학습률을 정할 때 너무 작지도 너무 크지도 않은 적절한 값을 지정해야합니다.  \n",
    "\n",
    "> ***하이퍼파라미터(Hyperparameter)란?***  \n",
    "> 하이퍼파라미터란 신경망의 매개변수와 다르게 신경망 학습을 '자동'으로 획득되는 매개변수가 아닌 직접 지정해주는 매개변수입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 신경망에서의 기울기\n",
    "자 그러면 이제 신경망에서 그레디언트를 구하는 코드를 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.82793655  0.02472479  0.0550517 ]\n",
      " [-2.54091071 -0.7315266   1.13832066]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x): # Output activation_function\n",
    "   c = np.max(x)\n",
    "   exp_x = np.exp(x-c)\n",
    "   return exp_x / np.sum(exp_x)\n",
    "\n",
    "def cross_entropy_error(y_hat,y): # loss function\n",
    "\n",
    "   if y.ndim == 1:\n",
    "      y = y.reshape(1,y.shape[0])\n",
    "\n",
    "   delta=1e-7\n",
    "   return np.sum(y*np.log(y_hat+delta))\n",
    "\n",
    "\n",
    "class SimpleNet: # Neural Network\n",
    "   def __init__(self):\n",
    "      self.W = np.random.randn(2,3) # np.random_randn -> 지정한 shape에 맞춰 정규분포에서 추출한 numpy array를 반환\n",
    "\n",
    "   def predict(self,x):\n",
    "      return np.dot(x,self.W) \n",
    "   \n",
    "   def loss(self,x,y):\n",
    "      z = self.predict(x)\n",
    "      y_hat = softmax(z)\n",
    "      loss = cross_entropy_error(y_hat,y)\n",
    "\n",
    "      return loss\n",
    "   \n",
    "net = SimpleNet()\n",
    "print(net.W)\n",
    "   \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.38358156 -0.64353907  1.05751961] (3,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6,0.9]) \n",
    "p = net.predict(x) # 행렬 곱 (2,) @ (2,3) -> (1,3)\n",
    "print(p,p.shape)\n",
    "print(np.argmax(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.17753766292828377\n"
     ]
    }
   ],
   "source": [
    "y = np.array([0,0,1]) # one-hot\n",
    "print(net.loss(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02321205 0.32202098]\n"
     ]
    }
   ],
   "source": [
    "def f(W): # Loss function\n",
    "    return net.loss(x,y)\n",
    "\n",
    "dW = numerical_diff(f,x) # 중앙차분\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
