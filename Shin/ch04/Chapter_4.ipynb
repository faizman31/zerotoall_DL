{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 Part에서는 **신경망 학습**에 대해 알아보겠습니다 **학습**이란 훈련 데이터로 부터 최적의 매개변수 값을 자동으로 획득하는 것입니다.\n",
    "이렇게 이야기하니 너무 딱딱하고 어려운데 쉽게 이야기하면 훈련 데이터를 통해서 우리가 해결하려고 하는 문제를 잘 풀 수 있는 파라미터(Weight,bias)를 찾는 \n",
    "과정입니다. 그렇다면 최적의 파라미터값을 찾아가는 기준은 무엇일까요? 바로 **손실함수** 입니다. 이번 장에서는 **손실함수**를 사용하여 최적의 파라미터 값을 찾는 \n",
    "**학습** 에 관련된 부분을 학습합니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 데이터에서 학습한다!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 데이터 주도 학습"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 신경망 학습을 본격적으로 알아보기 전에 한가지 기억해야할 것이 하나 있습니다. **기계학습에서 가장 중요한것은 데이터라는 점!** 입니다. \n",
    "기계학습은 데이터는 수집한 데이터로부터 패턴을 찾습니다. 예를 들어 숫자 5 사진을 보고 5를 분류하는 모델을 생성한다고 생각합니다.\n",
    "5라는 숫자는 아래쪽이 둥글다,왼쪽 위 모서리가 각져있다 등 다양한 특징들을 찾을 수 있습니다. 이러한 다양한 특징들을 학습한다면 \n",
    "숫자 5를 분류하는 모델을 생성할 수 있습니다. **특징(Feature)** 이란 입력된 데이터에서 중요한 데이터를 의미합니다. \n",
    " \n",
    "기계학습은 이러한 특징들을 넣어 학습합니다. 만약 기계학습에 문제에 적절한 **특징(Feature)** 을 넣지 않는다면 좋은 결과를 얻을 수 없습니다. 이처럼 기계학습에서는\n",
    "중요한 특징들을 추출하는 것이 중요합니다. 반면 딥러닝은 입력데이터를 그대로 입력받아 스스로 **특징(Feature)** 를 추출하여 End-to-End 방식으로 학습합니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 훈련 데이터와 시험 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 기계학습이라는 큰 틀 안에 딥러닝이 존재합니다. 그래서 편의상 기계학습과 딥러닝이 같은 의미로 나올 수 있으니 참고해주세요.  \n",
    "\n",
    "기계학습 문제는 훈련 데이터와 테스트 데이터로 나누어 학습과 실험을 진행합니다. 훈련 데이터로만 학습하여 훈련 데이터에 대한 최적의 매개변수 값을 찾고\n",
    "테스트 데이터를 통해 훈련한 모델을 평가합니다. 왜 나누어서 학습과 실험을 진행하는 걸까요? 바로 범용능력을 제대로 측정하기 위해서입니다.  \n",
    "범용능력이란? -> 한번도 보지 못한 데이터에 대해서 올바르게 풀어내는 능력  \n",
    "만약 테스트 데이터까지 학습에 사용한다면 범용능력을 평가하기가 어려워지기 때문에 나누는 것입니다.\n",
    "그리고 한가지 더 알아야할 것이 있습니다. 바로 **오버피팅(Overfitting)** 입니다. 모델이 문제를 풀어내는 능력을 기른것이 아니라 \n",
    "정답을 모두 외워버렷다고 이해하시면 됩니다. 이런 경우 당연히 범용능력이 떨어지겠죠? 이처럼 기계학습에서는 범용능력을 키우고 오버피팅을 피하는것이 중요한 과제입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 손실함수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습에서는 현재의 상태를 \"하나의 지표\"로 표현하고 그 지표를 가장 좋게하는 최적의 가중치 매개변수 값을 탐색하는 것입니다.  \n",
    "이때 신경망 학습에서 사용되는 \"하나의 지표\"를 **손실함수(Loss function)** 라고 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 오차제곱합(Sum of Squares Error ,SSE)\n",
    "- 오차제곱합은 가장 많이 쓰이는 손실 함수중 하나입니다.\n",
    "- 모델이 추론한 값과 실제 label 값을 빼서 제곱한 것들의 합을 1/2로 나누어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_error(y_hat,y):\n",
    "    return np.sum((y_hat-y)**2) * 0.5 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차제곱합을 식을 기반으로 구현하였습니다. 그렇다면 이것이 어떻게 작동되는지 한번 확인해보기 전에  \n",
    "원-핫 인코딩에대해서 이야기하고 바로 예제를 진행해보겠습니다.  \n",
    "원-핫 인코딩은 한 원소만 1로 표현하고 나머지는 0으로 표현하는 인코딩 방법입니다.   \n",
    "만약 [0,0,1,0] 인 경우에는 숫자 0~3을 표현한다고했을때 2에 해당하는 원-핫 인코딩인것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat1 : 0.16\n",
      "y_hat2 : 0.56\n"
     ]
    }
   ],
   "source": [
    "y = np.array([0,0,1,0,0]) # one-hot -> 2\n",
    "\n",
    "# softmax를 통과한 출력 -> 3번째 인덱스의 값이 가장 큼 -> 2\n",
    "y_hat1 = np.array([0.1,0.2,0.5,0.1,0.1])\n",
    "\n",
    "# 0번째 인덱스의 값이 가장 큼 -> 0\n",
    "y_hat2 = np.array([0.5,0.2,0.1,0.1,0.1] )\n",
    "\n",
    "print('y_hat1 :',sum_of_squares_error(y_hat1,y))\n",
    "print('y_hat2 :',sum_of_squares_error(y_hat2,y))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차제곱합으로 손실값을 계산한결과 label와 index와 추론값의 가장 큰 값의 index가 동일한 출력의 손실이 더 적은 것으로 확인이 가능합니다.  \n",
    "그리고 label의 해당하는 index의 값을 크게 만드는 방향으로 학습하면 손실함수의 값이 감소하는 방향일 것입니다.   \n",
    "이처럼 손실함수는 신경망 학습의 하나의 지표로서 활용이 가능합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차(Cross Entropy Error,CEE)\n",
    "- 오차제곱합와 같이 많이 사용되는 손실함수 입니다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_hat,y):\n",
    "    delta = 1e-7 # 값이 너무 작아지는 것을 방지\n",
    "    return -np.sum(y*np.log(y_hat+delta)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "식을 그림을 그리면서 하나씩 설명하겠습니다. 먼저 y가 원-핫 인코딩으로 구현이 되어있다고 가정하고, \n",
    "y_hat은 최종 출력계층의 활성화 함수 Softmax 를 통과했다고 가정해보겠습니다. 그러면 아래 그림과 같은 손실함수 값을 얻을 수 있고\n",
    "로그함수를 비교해봣을때 label에 해당하는 index의 출력값을 키우는 방향으로 학습하면 된다는 방향을 확인해 볼 수 있습니다.  \n",
    "\n",
    "이제 똑같은 예제로 한번 손실함수의 값을 확인해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat1 : 0.6931469805599654\n",
      "y_hat2 : 2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "y = np.array([0,0,1,0,0]) # one-hot -> 2\n",
    "\n",
    "# softmax를 통과한 출력 -> 3번째 인덱스의 값이 가장 큼 -> 2\n",
    "y_hat1 = np.array([0.1,0.2,0.5,0.1,0.1])\n",
    "\n",
    "# 0번째 인덱스의 값이 가장 큼 -> 0\n",
    "y_hat2 = np.array([0.5,0.2,0.1,0.1,0.1] )\n",
    "\n",
    "print('y_hat1 :',cross_entropy_error(y_hat1,y))\n",
    "print('y_hat2 :',cross_entropy_error(y_hat2,y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차제곱합때보다 좀 더 확연하게 손실함수 값의 차이가 나는 것을 볼 수 있습니다. \n",
    "실제로 오차제곱합은 주로 Regression Task에서 사용하고  \n",
    "교차 엔트로피의 경우 Classification Task에서 손실함수로 많이 사용됩니다.\n",
    "\n",
    "> 추가로 보통 오차제곱합과 교차엔트로피 등 손실함수를 사용할 때는 데이터의 갯수 N을 최종값에 나누어 평균 손실 함수를 구합니다.  \n",
    "> 평균 손실 함수를 구하는 이유는 데이터 개수와 상관없이 언제든 통일된 지표를 얻기 위함입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습\n",
    "- 미니배치 학습 - 훈련데이터로 부터 일부를 골라 학습하는 방법\n",
    "- 왜 미니배치 학습을 사용할까? \n",
    "  - 데이터가 한번에 처리하기 너무 큰 양일 수 있어서 \n",
    "  - 모든 데이터를 학습에 반복하여 사용할 경우 연산이 많아 효율적이지 않을 수 있어서\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 이유로 미니배치학습을 많이 사용합니다. 그러면 이제 미니배치를 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train : (60000, 784)\n",
      "x_test : (10000, 784)\n",
      "y_train : (60000, 10)\n",
      "y_test : (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,y_train) , (x_test,y_test) = load_mnist(normalize=True,flatten=True,one_hot_label=True)\n",
    "\n",
    "print(\"x_train :\",x_train.shape) # 학습 이미지 \n",
    "print(\"x_test :\",x_test.shape) # 테스트 이미지\n",
    "print(\"y_train :\",y_train.shape) # 학습 이미지 abel\n",
    "print(\"y_test :\",y_test.shape) # 테스트 이미지 label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치를 구현하기 위한 함수로 `np.random.choice` 를 사용할 수 있습니다. 이 함수는 지정된 범위안에서 지정한 만큼의 데이터를\n",
    "무작위로 뽑아줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|x_batch|: (10, 784)\n",
      "|y_batch|: (10, 10)\n"
     ]
    }
   ],
   "source": [
    "train_cnt = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_cnt,batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "y_batch = y_train[batch_mask]\n",
    "\n",
    "print(\"|x_batch|:\",x_batch.shape)\n",
    "print(\"|y_batch|:\",y_batch.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기\n",
    "미니배치를 지원하는 교차 엔트로피 오차를 구현해보도록 하겠습니다. 그리 어렵지 않습니다 추가된 개념이라곤 데이터가 미니배치 단위로 들어오는 것 뿐입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_hat,y):\n",
    "\n",
    "    if y.ndim == 1:\n",
    "        y_hat = y_hat.reshape(1,y_hat.shape[0])\n",
    "        y = y.reshape(1,y.shape[0])\n",
    "\n",
    "    batch_size = y_hat.shape[0]\n",
    "    delta = 1e-7\n",
    "    return np.sum(y*np.log(y_hat+delta)) / batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음은 label이 원-핫 인코딩으로 이루어져있지 않을때의 배치용 교차 엔트로피를 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_hat,y):\n",
    "\n",
    "    if y.ndim == 1:\n",
    "        y_hat = y_hat.reshape(1,y_hat.shape[0])\n",
    "        y = y.reshape(1,y.shape[0])\n",
    "\n",
    "    batch_size = y_hat.shape[0]\n",
    "    delta = 1e-7\n",
    "    return np.sum(np.log(y_hat[np.arange(batch_size),y]+delta)) / batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.sum(y*np.log(y_hat+delta))` 에서 `np.sum(np.log(y_hat[np.arange(batch_size),y]+delta))`로 바뀐 이유는   \n",
    "label(y)의 경우 원-핫 인코딩 벡터일때는 정답에 해당하는 label index를 알기위해 사용됬지만 원-핫 인코딩이 아닌 2,3 과 같이 숫자인 경우에  \n",
    "이를 곱해줄 이유가 없어졌기 떄문입니다 추론된 값(y_hat)에서 몇번째 데이터의 label index에 값만 가져오면 되기 때문입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
