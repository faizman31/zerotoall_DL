# Chapter 4. 신경망 학습  

## 4.1 데이터에서 학습한다!
- 신경망의 특징 : 데이터를 보고 학습 -> **가중치 매개변수**의 값을 데이터를 보고 자동으로 결정  

### 4.1.1 데이터 주도 학습
- 기계학습에 생명 -> "데이터"
- 데이터에서 답을 찾고 패턴을 발견하고 데이터로 이야기를 만드는 것이 기계학습이다.
- 특징(Feature) : 입력 데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기
- 신경망은 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 'End-to-End'로 학습이 가능하다.  

### 4.1.2 훈련 데이터와 학습 데이터
- 기계학습 문제는 데이터를 훈련 데이터(Training Data)와 시험 데이터(Test Data)로 나누어 학습/실험을 수행
- 왜 훈련데이터와 시험데이터를 나누어야 할까? -> **범용 능력**을 제대로 평가하기 위해
- 범용능력이란? : 아직 보지 못한 데이터(훈련 데이터에 포함되어 있지 않는 데이터)로도 문제를 올바르게 풀어내는 능력
- 오버피팅(Overfitting)이란? : 한 데이터셋에만 지나치게 최적화된 상태  

## 4.2 손실함수
- 신경망 학습에서는 현재 상태를 '하나의 지표'로 표현합니다.
- 신경망 학습은 '하나의 지표'를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것
- 신경망 학습의 '하나의 지표' -> **손실 함수(Loss function)**  
### 4.2.1 오차제곱합(Sum of Squares for error,SSE)
- 오차제곱합 수식
$$ E = {1 \over 2} \sum (y_k - t_k)^2
$$
- 원-핫 인코딩이란? : 한 원소만 1로 하고 그 외는 0으로 나타내는 표기법
### 4.2.2 교차 엔트로피(Cross Entropy error,CEE)
- 교차 엔트로피 오차(Cross Entropy error,CEE)의 수식
$$ -\sum{t_k\log{y_t}}
$$  
### 4.2.3 미니배치 학습 
- 데이터의 개수와 관계없이 언제든 통일된 지표를 얻고 싶다면? -> 데이터의 갯수 N으로 나눔으로써 **"평균 손실 함수"** 를 구하면 된다.
- 미니배치(mini-batch) 학습 : 많은 데이터를 대상으로 데이터 일부를 추려 전체의 '근사치'로 이용하는 방법으로 훈련 데이터로부터 일부만을 골라 학습을 수행  
### 4.2.5 왜 손실 함수를 설정하는가?
- 손실 함수의 값을 사용하는 이유
    - 신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 작게 하는 매개변수 값을 찾습니다.
    - 이 때 매개변수의 미분을 계싼하고 그 미분값을 단서로 매개변수의 값을 갱신합니다.
    - 가중치 매개변수의 손실 함수의 미분이란 '가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하냐' 라는 의미입니다.
- 손실 함수가 아닌 정확도를 지표로 설정했다면?
    - 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신 할 수 없게 된다.
    - 정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않고 반응이 있더라고 그 값이 불연속적으로 갑자기 변화합니다.  
## 4.3 수치 미분
- 경사법에서는 **기울기(경사)** 값을 기준으로 나아갈 방향을 정합니다.  
### 4.3.1 미분
> 예제 : 10분에 2km를 달렸다고 했을 때 평균 속도는 2 / 10 = 0.2km/h 
- 미분은 **'특정 순간'**의 변화량을 뜻합니다. -> 한 순간의 변화량(어느 순간의 속도)
- 미분은 수치 미분과 차분하는 방법 두가지 방법이 존재합니다.
- 수치 미분의 단점 : 반올림 오차(rounding error)로 인해 최종 계산 결과에 오차가 생기게 합니다.
    - 반올림 오차(rounding error) : 작은 값(가령 소수점 8자리 이하)이 생략
- 수치 미분의 단점을 해결하기 위한 방법 : **중심 차분(== 중앙 차분)**
- 중앙 차분 : x를 중심으로 그 전후의 차분을 계산하는 방법
$$ {{f(x+h) - f(x-h)} \over {2h}}
$$