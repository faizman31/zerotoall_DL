# Chapter 4. 신경망 학습  

## 4.1 데이터에서 학습한다!
- 신경망의 특징 : 데이터를 보고 학습 -> **가중치 매개변수**의 값을 데이터를 보고 자동으로 결정  

### 4.1.1 데이터 주도 학습
- 기계학습에 생명 -> "데이터"
- 데이터에서 답을 찾고 패턴을 발견하고 데이터로 이야기를 만드는 것이 기계학습이다.
- 특징(Feature) : 입력 데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기
- 신경망은 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 'End-to-End'로 학습이 가능하다.  

### 4.1.2 훈련 데이터와 학습 데이터
- 기계학습 문제는 데이터를 훈련 데이터(Training Data)와 시험 데이터(Test Data)로 나누어 학습/실험을 수행
- 왜 훈련데이터와 시험데이터를 나누어야 할까? -> **범용 능력**을 제대로 평가하기 위해
- 범용능력이란? : 아직 보지 못한 데이터(훈련 데이터에 포함되어 있지 않는 데이터)로도 문제를 올바르게 풀어내는 능력
- 오버피팅(Overfitting)이란? : 한 데이터셋에만 지나치게 최적화된 상태  

## 4.2 손실함수
- 신경망 학습에서는 현재 상태를 '하나의 지표'로 표현합니다.
- 신경망 학습은 '하나의 지표'를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것
- 신경망 학습의 '하나의 지표' -> **손실 함수(Loss function)**  
### 4.2.1 오차제곱합(Sum of Squares for error,SSE)
- 오차제곱합 수식  
$$ E = {1 \over 2} \sum (y_k - t_k)^2
$$
- 원-핫 인코딩이란? : 한 원소만 1로 하고 그 외는 0으로 나타내는 표기법
### 4.2.2 교차 엔트로피(Cross Entropy error,CEE)
- 교차 엔트로피 오차(Cross Entropy error,CEE)의 수식
$$ -\sum{t_k\log{y_t}}
$$  
### 4.2.3 미니배치 학습 
- 데이터의 개수와 관계없이 언제든 통일된 지표를 얻고 싶다면? -> 데이터의 갯수 N으로 나눔으로써 **"평균 손실 함수"** 를 구하면 된다.
- 미니배치(mini-batch) 학습 : 많은 데이터를 대상으로 데이터 일부를 추려 전체의 '근사치'로 이용하는 방법으로 훈련 데이터로부터 일부만을 골라 학습을 수행  
### 4.2.5 왜 손실 함수를 설정하는가?
- 손실 함수의 값을 사용하는 이유
    - 신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 작게 하는 매개변수 값을 찾습니다.
    - 이 때 매개변수의 미분을 계싼하고 그 미분값을 단서로 매개변수의 값을 갱신합니다.
    - 가중치 매개변수의 손실 함수의 미분이란 '가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하냐' 라는 의미입니다.
- 손실 함수가 아닌 정확도를 지표로 설정했다면?
    - 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신 할 수 없게 된다.
    - 정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않고 반응이 있더라고 그 값이 불연속적으로 갑자기 변화합니다.  
## 4.3 수치 미분
- 경사법에서는 **기울기(경사)** 값을 기준으로 나아갈 방향을 정합니다.  
### 4.3.1 미분
> 예제 : 10분에 2km를 달렸다고 했을 때 평균 속도는 2 / 10 = 0.2km/h 
- 미분은 **'특정 순간'**의 변화량을 뜻합니다. -> 한 순간의 변화량(어느 순간의 속도)
- 미분은 수치 미분과 차분하는 방법 두가지 방법이 존재합니다.
- 수치 미분의 단점 : 반올림 오차(rounding error)로 인해 최종 계산 결과에 오차가 생기게 합니다.
    - 반올림 오차(rounding error) : 작은 값(가령 소수점 8자리 이하)이 생략
- 수치 미분의 단점을 해결하기 위한 방법 : **중심 차분(== 중앙 차분)**
- 중앙 차분 : x를 중심으로 그 전후의 차분을 계산하는 방법  
$$ {{f(x+h) - f(x-h)} \over {2h}}
$$  
### 4.3.2 편미분
- 편미분 : 변수가 여럿인 함수에 대한 미분
- ${{\partial f}\over{\partial x_0}},{{\partial f}\over{\partial x_1}}$ 같이 표현
- 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구합니다.
- 여러 변수중 **목표 변수 하나**에 초점을 맞추고 다른 변수는 값을 고정합니다.  
## 4.4 기울기(Gradient)
- Gradient란? : 모든 변수의 편미분을 벡터로 정리한 것
- 기울기(Gradient)가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향입니다.  
### 4.4.1 경사법
- 신경망은 최적의 매개변수(가중치와 편향)를 학습 시에 찾는것이 목표입니다.
- 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값이지만 일반적으로 손실 함수를 매우 복잡하여 최솟값이 어디인지 짐작하기가 어렵습니다.
- 그래서 기울기를 이용해 함수의 최솟값을 찾는 것이 경사법 입니다.
> 함수의 값을 낮추는 방안을 제시하는 지표 -> 기울기
- 경사법은 기울어진 방향으로 일정 거리만큼 이동하고 이를 반복하면서 함수의 값을 점차 줄이는 것이 경사법(gradient method)입니다.  

$$ x_0 = x_0 - /eta{{\partial f}\over {\partial x_0}} \\ x_1 = x_1 - \eta {{\partial f}\over {\partial x_1}}
$$
- 학습률($\eta$,learning rate) : 갱신하는 양을 나타냄 -> 한번 학습으로 얼마만큼 학습할 것인지 즉 매개변수의 값을 얼마나 갱신할건지를 정함  
## 4.5 학습 알고리즘 구현하기
- 신경망의 학습 절차
    - 전제 : 신경망에는 가중치와 편향이 있고 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 학습이라고 한다.
    - 1단계 - 미니배치
        - 훈련 데이터 중 일부를 무작위로 가져옵니다. 
        - 이렇게 선별된 데이터를 미니배치라고 합니다.
        - 미니배치의 손실 함수의 값을 줄이는 것이 목표입니다.
    - 2단계 - 기울기 산출
        - 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구합니다.
        - 기울기(Gradient)는 손실 함수의 값을 가장 작게 하는 방향을 제시합니다.
    - 3단계 - 가중치 갱신
        - 가중치 매개변수를 기울기 방향으로 아주 조금 갱신합니다.(갱신의 정도는 "학습률:learning rate"로 결정됩니다.)
    - 4단계 - 반복
        - 1~3단계를 반복합니다.
- 확률적 경사 하강법(Stochastic Gradient Descent) : 확률적으로 무작위로 골라낸 데이터에 대해 수행하는 경사 하강법  



